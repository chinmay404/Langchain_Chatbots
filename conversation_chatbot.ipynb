{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key = os.getenv('GOOGLE_API_KEY')\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "persist_directory=\"./chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.1, api_key=key)\n",
    "llm = Ollama(model=\"llama3:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain is a library designed to enable developers to leverage the power of multiple natural language processing (NLP) models, including large language models like Transformers and GPT-4. It provides an extensible interface for interacting with these models, making it easier to build AI applications that incorporate NLP functionality. LangChain acts as the intermediary between the client and the underlying model, allowing developers to control the flow of text through their application without having to worry about model-specific details.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inovke(\"What Is LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't remember what question or instruction was given to me. Can you please provide more context about the scenario or situation that led to this interaction?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory = ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'input'] template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"My interest is to explore the options of scaling Ethereum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your interest was in exploring options for scaling Ethereum. This means that you're interested in the process of increasing or improving how Ethereum handles transactions and interactions with other blockchain networks, especially when it comes to handling large amounts of data. The goal is to make the Ethereum platform more efficient, allowing it to handle larger numbers of transactions at once without slowing down.\\nTo scale Ethereum, there are several options you can consider:\\n\\n1. **EVM Upgrade**: This involves modifying the core codebase of the Ethereum Virtual Machine (EVM) itself. Upgrades may introduce changes that improve performance and enable additional features or functionalities.\\n\\n2. **Layer 2 Solutions**: These refer to protocols or solutions that enhance Ethereum's scalability by running transactions on a separate layer from the main network. Examples include Sharding, Sidechains, and Layer 2-based scaling solutions like Arbitrum, Optimism, or Polygon.\\n\\n3. **Smart Contract Enhancements**: The addition of smart contracts (automated agreements) can improve transaction speed and reduce fees by automating certain actions within contracts, reducing the need for manual approval in transactions.\\n\\n4. **Blockchain Interoperability**: This involves enabling Ethereum to interact with other blockchain networks without compromising its own security or performance. This approach is beneficial when considering cross-chain applications where data needs to be transferred between different blockchains efficiently.\\n\\n5. **Ethereum Virtual Machine Optimization**: While not strictly a scaling option, improving the efficiency of the EVM can help reduce gas fees and improve transaction speed. Optimizing the bytecode (code) and reducing the number of gas used by transactions can be beneficial in this regard.\\n\\n6. **Distributed Ledger Technology (DLT)**: DLTs such as Hyperledger Sawtooth or Ethereum's own Solana offer alternative approaches to blockchain technology, potentially scaling Ethereum even further without significantly changing its core codebase.\\n\\nThese options and others may be explored depending on your specific needs and goals for scaling Ethereum.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"What just i said you what was my intrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My interest is to explore the options of scaling Ethereum\n",
      "AI: Sure, I can help you understand what options there are for scaling Ethereum. What specifically do you need to know about this?\n",
      "Human: What just i said you what was my intrest\n",
      "AI: Your interest was in exploring options for scaling Ethereum. This means that you're interested in the process of increasing or improving how Ethereum handles transactions and interactions with other blockchain networks, especially when it comes to handling large amounts of data. The goal is to make the Ethereum platform more efficient, allowing it to handle larger numbers of transactions at once without slowing down.\n",
      "To scale Ethereum, there are several options you can consider:\n",
      "\n",
      "1. **EVM Upgrade**: This involves modifying the core codebase of the Ethereum Virtual Machine (EVM) itself. Upgrades may introduce changes that improve performance and enable additional features or functionalities.\n",
      "\n",
      "2. **Layer 2 Solutions**: These refer to protocols or solutions that enhance Ethereum's scalability by running transactions on a separate layer from the main network. Examples include Sharding, Sidechains, and Layer 2-based scaling solutions like Arbitrum, Optimism, or Polygon.\n",
      "\n",
      "3. **Smart Contract Enhancements**: The addition of smart contracts (automated agreements) can improve transaction speed and reduce fees by automating certain actions within contracts, reducing the need for manual approval in transactions.\n",
      "\n",
      "4. **Blockchain Interoperability**: This involves enabling Ethereum to interact with other blockchain networks without compromising its own security or performance. This approach is beneficial when considering cross-chain applications where data needs to be transferred between different blockchains efficiently.\n",
      "\n",
      "5. **Ethereum Virtual Machine Optimization**: While not strictly a scaling option, improving the efficiency of the EVM can help reduce gas fees and improve transaction speed. Optimizing the bytecode (code) and reducing the number of gas used by transactions can be beneficial in this regard.\n",
      "\n",
      "6. **Distributed Ledger Technology (DLT)**: DLTs such as Hyperledger Sawtooth or Ethereum's own Solana offer alternative approaches to blockchain technology, potentially scaling Ethereum even further without significantly changing its core codebase.\n",
      "\n",
      "These options and others may be explored depending on your specific needs and goals for scaling Ethereum.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory = ConversationBufferWindowMemory(k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [\n",
    "    \"Im Chinmay Pisal Remeber My name\",\n",
    "    \"i have intrest in coding\",\n",
    "    \"What is my name\",\n",
    "    \"What is my intrest \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Im Chinmay Pisal Remeber My name\n",
      "Result: Sure, I remember you. Is there anything specific you would like to know or discuss about your interest in coding?\n",
      "\n",
      "Query: i have intrest in coding\n",
      "Result: That's great! If you're interested in learning more about coding or if you need help with any specific project, feel free to ask me for assistance. How can I help you today?\n",
      "\n",
      "Query: What is my name\n",
      "Result: Your name is Chinmay Pisal. Is there anything else you would like to know?\n",
      "\n",
      "Query: What is my intrest \n",
      "Result: You have mentioned that coding is one of your interests, and you're interested in learning more about it. How can I help with that? Are you looking for resources or tutorials on coding? Or do you need advice on starting a project or improving your skills?\n",
      "\n",
      "Conversation Memory Buffer\n",
      "Human: Im Chinmay Pisal Remeber My name\n",
      "AI: Sure, I remember you. How else can I help?\n",
      "Human: Im Chinmay Pisal Remeber My name\n",
      "AI: Sure, I remember you. Is there anything specific you would like to know or discuss about your interest in coding?\n",
      "Human: i have intrest in coding\n",
      "AI: That's great! If you're interested in learning more about coding or if you need help with any specific project, feel free to ask me for assistance. How can I help you today?\n",
      "Human: What is my name\n",
      "AI: Your name is Chinmay Pisal. Is there anything else you would like to know?\n",
      "Human: What is my intrest \n",
      "AI: You have mentioned that coding is one of your interests, and you're interested in learning more about it. How can I help with that? Are you looking for resources or tutorials on coding? Or do you need advice on starting a project or improving your skills?\n"
     ]
    }
   ],
   "source": [
    "chat = []\n",
    "for querry in q:\n",
    "    print(f'Query: {querry}')\n",
    "    output = conversation.run(querry)\n",
    "    print(f'Result: {output}')\n",
    "    print('')\n",
    "    chat.append(output)\n",
    "chat \n",
    "print(\"** Conversation Memory Buffer : \")\n",
    "print(conversation.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['new_lines', 'summary'] template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:'\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
    "print(conversation.memory.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Im Chinmay Pisal Remeber My name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Conversation Memory Update **: \n",
      "New summary:\n",
      "The human introduces himself as Chinmay Pisal and asks the AI to remember his name. The AI responds positively, assuring Chinmay that it will remember his name and emphasizing that remembering is crucial to its learning process. The AI then politely inquires if Chinmay would like to discuss anything else. \n",
      "\n",
      "\n",
      "Result: It's nice to meet you, Chinmay Pisal! I will do my best to remember your name.  I'm still learning, and remembering things is an important part of that. \n",
      "\n",
      "Is there anything else you'd like to talk about today? ðŸ˜Š \n",
      "\n",
      "\n",
      "\n",
      "Query: i have intrest in coding\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "New summary:\n",
      "The human introduces himself as Chinmay Pisal and asks the AI to remember his name. The AI responds positively, assuring Chinmay that it will remember his name and emphasizing that remembering is crucial to its learning process. The AI then politely inquires if Chinmay would like to discuss anything else. Chinmay shares his interest in coding. The AI expresses enthusiasm, recognizing coding as a fascinating field. It then inquires about Chinmay's specific interests in coding, asking about his favorite programming language and his experience level with software development, expressing eagerness to learn more. \n",
      "\n",
      "\n",
      "Result: That's fantastic, Chinmay! Coding is a fascinating field. What is it about coding that interests you?  Do you have a favorite programming language, or are you just starting to explore the world of software development? I'm eager to hear more about your interests! ðŸ˜Š \n",
      "\n",
      "\n",
      "\n",
      "Query: What is my name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Conversation Memory Update **: \n",
      "New summary:\n",
      "The human introduces himself as Chinmay Pisal and asks the AI to remember his name. The AI responds positively, assuring Chinmay that it will remember his name and emphasizing that remembering is crucial to its learning process. The AI then politely inquires if Chinmay would like to discuss anything else. Chinmay shares his interest in coding. The AI expresses enthusiasm, recognizing coding as a fascinating field. It then inquires about Chinmay's specific interests in coding, asking about his favorite programming language and his experience level with software development, expressing eagerness to learn more. **Chinmay then tests the AI by asking it to recall his name. The AI successfully remembers his name, replying with \"Your name is Chinmay Pisal! ðŸ˜Š\"  It reaffirms its commitment to remembering names as a learning mechanism and then politely prompts Chinmay to continue the conversation by asking \"What else would you like to talk about?\"** \n",
      "\n",
      "\n",
      "Result: Your name is Chinmay Pisal! ðŸ˜Š  I'm really trying to be good at remembering names, it helps me learn.  What else would you like to talk about? \n",
      "\n",
      "\n",
      "\n",
      "Query: What is my intrest \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m querry \u001b[38;5;129;01min\u001b[39;00m q:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquerry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mrun(querry)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m** Conversation Memory Update **: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconversation\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain\\chains\\base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    384\u001b[0m     inputs,\n\u001b[0;32m    385\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    386\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    387\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    388\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain\\chains\\llm.py:128\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    125\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    126\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 128\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain\\chains\\llm.py:140\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    138\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    141\u001b[0m         prompts,\n\u001b[0;32m    142\u001b[0m         stop,\n\u001b[0;32m    143\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    148\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    149\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:698\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    692\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    696\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    697\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:555\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    554\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 555\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    556\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    557\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    559\u001b[0m ]\n\u001b[0;32m    560\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:545\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    544\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 545\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    546\u001b[0m                 m,\n\u001b[0;32m    547\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    548\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    550\u001b[0m             )\n\u001b[0;32m    551\u001b[0m         )\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:770\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 770\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    771\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    772\u001b[0m         )\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:546\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    536\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    540\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    541\u001b[0m     params, chat, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(\n\u001b[0;32m    542\u001b[0m         messages,\n\u001b[0;32m    543\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    545\u001b[0m     )\n\u001b[1;32m--> 546\u001b[0m     response: genai\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mGenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m    547\u001b[0m         content\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    549\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39mchat\u001b[38;5;241m.\u001b[39msend_message,\n\u001b[0;32m    550\u001b[0m     )\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:153\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\tenacity\\__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    388\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep(do)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32md:\\anaconda\\envs\\aienv\\Lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(seconds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat = []\n",
    "for querry in q:\n",
    "    print(f'\\nQuery: {querry}')\n",
    "    output = conversation.invoke(querry)\n",
    "    print(f\"\\n** Conversation Memory Update **: \\n{conversation.memory.buffer}\")\n",
    "    print(f'\\nResult: {output}')\n",
    "    print('')\n",
    "    chat.append(output)\n",
    "print(\"** Conversation Memory Buffer : \")\n",
    "print(f\"Total Conversation Memory : \\n{conversation.memory.buffer}\")\n",
    "chat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding manual enterd Human Message into Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\aienv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Hello! How can I assist you today?\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "The human asks how the AI can assist them today. The AI responds by greeting and asking if they need assistance with something specific.\n",
      "\n",
      "\n",
      "Human : my name is Chinmay Pisal You can call me Chinmay\n",
      "\n",
      "AI: Hello Chinmay, nice to meet you! I'm an artificial intelligence named Ava, and I'm here to help assist you today. How may I be of service?\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "The human introduces themselves as Chinmay Pisal and greets the AI by calling it Chinmay, then asks if it can assist them. The AI responds with a friendly greeting and offers its assistance in today's conversation. END OF EXAMPLE\n",
      "\n",
      "\n",
      "Human : well i want to know about llm in ver short\n",
      "\n",
      "AI: Hello! It seems you are referring to an abbreviation or acronym for \"LLM,\" which stands for Master of Laws, typically abbreviated as LL.M., a graduate degree awarded by law schools that focuses on legal theory and practice. This degree can be obtained after completing an undergraduate program in law or another field.\n",
      "\n",
      "Some key things about LLM studies include:\n",
      "\n",
      "1. **Study of Law**: Typically includes coursework focused on various areas such as criminal law, constitutional law, civil procedure, administrative law, torts, and more.\n",
      "2. **Advanced Legal Studies**: Includes deeper study of legal theory, including jurisprudence and critical legal studies.\n",
      "3. **Professional Skills**: Training in areas like writing, research analysis, negotiation, and presentation skills to prepare students for a career as a lawyer or a legal professional.\n",
      "4. **Legal Research**: Techniques for conducting legal research to support the creation of arguments and the preparation of legal documents.\n",
      "\n",
      "LLM degrees are particularly sought after by those who wish to pursue careers in law but want a more specialized training compared to a Bachelor's degree in Law (B.L.) or Juris Doctorate (J.D.). They can also be considered as part of the admissions process for some law schools, which requires an LLM before students can apply to become members of the bar and practice law.\n",
      "\n",
      "For further information, you might want to visit websites like Law School Admission Council (LSAC) for more detailed study about LL.M programs offered in various countries.\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "The AI introduces Chinmay Pisal as Chinmay, then explains that an LLM stands for Master of Laws and is a graduate degree awarded by law schools. It includes coursework focused on various legal areas such as criminal law, constitutional law, civil procedure, administrative law, torts, and more. Some key things about LLMS include study of law (including advanced legal studies), professional skills in writing, research analysis, negotiation, and presentation, and techniques for conducting legal research to support arguments and prepare legal documents. The AI recommends visiting Law School Admission Council's website for more detailed information on LL.M programs offered in various countries.\n",
      "\n",
      "\n",
      "Human : thats great\n",
      "\n",
      "AI: Great! Is there anything else you'd like me to explain or help with? I'm here to assist you.\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "The AI introduces Chinmay Pisal as Chinmay and explains that an LLM stands for Master of Laws. It includes coursework focused on various legal areas such as criminal law, constitutional law, civil procedure, administrative law, torts, and more. The AI recommends visiting Law School Admission Council's website for more detailed information on LL.M programs offered in various countries.\n",
      "\n",
      "\n",
      "Human : well tell me meaning of my name\n",
      "\n",
      "AI: Your name, Chinmay Pisal, is a combination of your first name and surname. Chinmay is an Indian last name that means \"Chin may\" or \"May Chhin,\" which translates to \"dear heart\" in English. Pisal is a given name that means \"pupil\" or \"student.\" Therefore, your full name expresses the characteristics of someone who has learned from their education and is dedicated to furthering knowledge.\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "The AI introduces Chinmay Pisal as Chinmay and explains that an LLM stands for Master of Laws. It includes coursework focused on various legal areas such as criminal law, constitutional law, civil procedure, administrative law, torts, and more. The AI recommends visiting Law School Admission Council's website for more detailed information on LL.M programs offered in various countries.\n",
      "\n",
      "\n",
      "Human : well tell me meaning of my name\n",
      "\n",
      "AI: Your name is Chinmay Pisal, which means \"Chinamay\" or \"Pisal\", both being Indian names. It sounds like you might be a proud Indian or possibly from the place where it was given. How can I assist you further?\n",
      "\n",
      "** Conversation Memory Update **: \n",
      "The AI introduces Chinmay Pisal as Chinmay and explains that an LLM stands for Master of Laws, includes coursework focusing on various legal areas such as criminal law, constitutional law, civil procedure, administrative law, torts, etc., recommends visiting the Law School Admission Council's website for detailed LL.M programs offered in various countries. The user asks about meaning of his name, which is Chinmay Pisal, an Indian name derived from \"Chinamay\" or \"Pisal,\" both being Indian names.\n",
      "\n",
      "\n",
      "Human : /bye\n",
      "\n",
      " Conversastion Memory : \n",
      "The AI introduces Chinmay Pisal as Chinmay and explains that an LLM stands for Master of Laws, includes coursework focusing on various legal areas such as criminal law, constitutional law, civil procedure, administrative law, torts, etc., recommends visiting the Law School Admission Council's website for detailed LL.M programs offered in various countries. The user asks about meaning of his name, which is Chinmay Pisal, an Indian name derived from \"Chinamay\" or \"Pisal,\" both being Indian names.\n"
     ]
    }
   ],
   "source": [
    "chat = []\n",
    "user_input = \"\"\n",
    "while user_input.strip() != \"/bye\":\n",
    "    user_input = input('\\nHuman: ').strip() \n",
    "    print(f\"\\nHuman : {user_input}\")\n",
    "    if user_input == \"/bye\":\n",
    "        break  \n",
    "    output = conversation.run(user_input)\n",
    "    print(f'\\nAI: {output}')\n",
    "    print(f\"\\n** Conversation Memory Update **: \\n{conversation.memory.buffer}\")\n",
    "    print('')\n",
    "    chat.append(output)\n",
    "print(f\"\\n Conversastion Memory : \\n{conversation.memory.buffer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
